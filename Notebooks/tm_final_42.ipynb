{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a46d8b2",
   "metadata": {},
   "source": [
    "# Text Mining Project - Stock Sentiment - Final Notebook\n",
    "\n",
    "## *Predicting market behavior from tweets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927efb5f",
   "metadata": {},
   "source": [
    "### Group 42\n",
    "\n",
    "Carolina Pinto - 20240494 <br>\n",
    "Fábio dos Santos - 20240678 <br>\n",
    "Guilherme – 2024 <br>\n",
    "Mariana – 2024 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db6d4a",
   "metadata": {},
   "source": [
    "Remarks: <br>\n",
    "- This Notebook is done to be used in Google Colab.\n",
    "- This Notebook assumes you have train.csv and test.csv datasets in your Google Drive.\n",
    "- When loading the datasets please adapt the files locations to where you have the datasets located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e166c3",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. Import Libraries](#1-import-libraries)\n",
    "- [2. Data Integration](#2-data-integration)\n",
    "- [3. Corpus Split](3-corpus-split)\n",
    "- [4. Data Preprocessing](#4-data-preprocessing)\n",
    "- [5. Model Trainning](#5-model-trainning)\n",
    "- [6. Deployment](#6-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c79b9",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e808f68",
   "metadata": {},
   "source": [
    "`Step 1` Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10132104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pessoal\\Mestrado\\Semester_2\\Text_Mining\\TM_Project\\Text_Mining_Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Fábio Dos\n",
      "[nltk_data]     Santos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Fábio Dos\n",
      "[nltk_data]     Santos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Fábio Dos\n",
      "[nltk_data]     Santos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Fábio Dos\n",
      "[nltk_data]     Santos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efc79a",
   "metadata": {},
   "source": [
    "# 2. Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465687a8",
   "metadata": {},
   "source": [
    "Our best model is BART, because it is computationally expensive to run it on a laptop without a GPU our group runned this notebook in Google Colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7befce",
   "metadata": {},
   "source": [
    "`Step 2` Setup to run notebook in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22df3c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 14 23:59:03 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX330      WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   33C    P8    N/A /  N/A |     64MiB /  2048MiB |      2%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a6fb01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e435cd",
   "metadata": {},
   "source": [
    "Make sure you have the train and test datasets in your Google drive and then adapt the location of the file in the following code cell to meet the actual location where you have the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e310f8",
   "metadata": {},
   "source": [
    "`Step 3` Import the datasets __train.csv__ and __test.csv__ using the method **read_csv()** from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/train.csv')\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560be024",
   "metadata": {},
   "source": [
    "`Step 4` Check the first 10 rows of the datasets to verify the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df949314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$FTI - TechnipFMC downgraded at Berenberg but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$GM - GM loses a bull https://t.co/tdUfG5HbXy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$GM: Deutsche Bank cuts to Hold https://t.co/7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$GTT: Cowen cuts to Market Perform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$HNHAF $HNHPD $AAPL - Trendforce cuts iPhone e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  $BYND - JPMorgan reels in expectations on Beyo...      0\n",
       "1  $CCL $RCL - Nomura points to bookings weakness...      0\n",
       "2  $CX - Cemex cut at Credit Suisse, J.P. Morgan ...      0\n",
       "3  $ESS: BTIG Research cuts to Neutral https://t....      0\n",
       "4  $FNKO - Funko slides after Piper Jaffray PT cu...      0\n",
       "5  $FTI - TechnipFMC downgraded at Berenberg but ...      0\n",
       "6      $GM - GM loses a bull https://t.co/tdUfG5HbXy      0\n",
       "7  $GM: Deutsche Bank cuts to Hold https://t.co/7...      0\n",
       "8                 $GTT: Cowen cuts to Market Perform      0\n",
       "9  $HNHAF $HNHPD $AAPL - Trendforce cuts iPhone e...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30196a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ETF assets to surge tenfold in 10 years to $50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Here’s What Hedge Funds Think Evolution Petrol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$PVH - Phillips-Van Heusen Q3 2020 Earnings Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>China is in the process of waiving retaliatory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Highlight: “When growth is scarce, investors s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Marvell Technology (MRVL) Gains As Market Dips...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>UPDATE 1-Italian airline Alitalia's rescue in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>why macro funds are shutting down left and rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Uber's arrival caused binge drinking to increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>New Dungeons &amp; Dragons game announced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0  ETF assets to surge tenfold in 10 years to $50...\n",
       "1   1  Here’s What Hedge Funds Think Evolution Petrol...\n",
       "2   2  $PVH - Phillips-Van Heusen Q3 2020 Earnings Pr...\n",
       "3   3  China is in the process of waiving retaliatory...\n",
       "4   4  Highlight: “When growth is scarce, investors s...\n",
       "5   5  Marvell Technology (MRVL) Gains As Market Dips...\n",
       "6   6  UPDATE 1-Italian airline Alitalia's rescue in ...\n",
       "7   7  why macro funds are shutting down left and rig...\n",
       "8   8  Uber's arrival caused binge drinking to increa...\n",
       "9   9              New Dungeons & Dragons game announced"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f80769",
   "metadata": {},
   "source": [
    "# 3. Corpus Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba2d79",
   "metadata": {},
   "source": [
    "Since our corpus have less than 10000 rows we will split it in train, validation and test in a 80%/10%/10% split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16aab1",
   "metadata": {},
   "source": [
    "`Step 5` Create a copy of the original dataframe named **data_train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd59323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>The Week's Gainers and Losers on the Stoxx Eur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>Tupperware Brands among consumer gainers; Unil...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>vTv Therapeutics leads healthcare gainers; Myo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>WORK, XPO, PYX and AMKR among after hour movers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>YNDX, I, QD and OESX among tech movers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9543 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     $BYND - JPMorgan reels in expectations on Beyo...      0\n",
       "1     $CCL $RCL - Nomura points to bookings weakness...      0\n",
       "2     $CX - Cemex cut at Credit Suisse, J.P. Morgan ...      0\n",
       "3     $ESS: BTIG Research cuts to Neutral https://t....      0\n",
       "4     $FNKO - Funko slides after Piper Jaffray PT cu...      0\n",
       "...                                                 ...    ...\n",
       "9538  The Week's Gainers and Losers on the Stoxx Eur...      2\n",
       "9539  Tupperware Brands among consumer gainers; Unil...      2\n",
       "9540  vTv Therapeutics leads healthcare gainers; Myo...      2\n",
       "9541    WORK, XPO, PYX and AMKR among after hour movers      2\n",
       "9542             YNDX, I, QD and OESX among tech movers      2\n",
       "\n",
       "[9543 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = deepcopy(df_train)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f75b6",
   "metadata": {},
   "source": [
    "__`Step 6`__ Create a varaible called `X` that store the values of the input features and `y` that stores the values of the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57099d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.drop(columns=['label'], axis =1)\n",
    "y = data_train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d1b4d",
   "metadata": {},
   "source": [
    "__`Step 7`__ Split the data in train and validation set in a 80/20 split, with random_state = 42, stratification by y and with shuffle of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23c3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y,\n",
    "                                                    shuffle=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb30ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,\n",
    "                                                    test_size=1/9,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_train_val,\n",
    "                                                    shuffle=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749531ff",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c37c31",
   "metadata": {},
   "source": [
    "__`Step 8`__ Create a function to do the data preprocessing. It includes:<br>\n",
    "|Preprocessement                                  |\n",
    "|------------------------------------|\n",
    "| Lowercasing |\n",
    "| Removes Emojis   |\n",
    "| Remove unknown character �           |\n",
    "| Removes Regular Unwanted Expressions   |\n",
    "| Remove Punctuation           |\n",
    "| Tokenization |\n",
    "| Remove Stop Words                    |\n",
    "| Lemmatization |\n",
    "| Stemming                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc6336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(text_list, lemma = None, stemmer = None, word2vec=False):\n",
    "    \"\"\"\n",
    "    Return the prepocessed text in a list \"updates\".\n",
    "\n",
    "    Parameters:\n",
    "    text_list : list to be preprocessed\n",
    "    use_lemmatize : bool, optional\n",
    "        If True, applies lemmatization to the tokens. Default is True.\n",
    "    use_stemmer : bool, optional\n",
    "        If True, applies stemming to the tokens. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    updates = []\n",
    "\n",
    "    for j in tqdm(text_list):\n",
    "\n",
    "        text = j\n",
    "\n",
    "        # Lower case text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove emojis\n",
    "        text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "\n",
    "        # Remove unknown character �\n",
    "        text = text.replace(\"�\", \"\")\n",
    "\n",
    "        # Remove Regular Unwanted Expressions\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Remove Punctuation\n",
    "        text = re.sub(rf\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        #Remove Stopwords\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        #Lemmatize\n",
    "        if lemma:\n",
    "            tokens = [lemma.lemmatize(word) for word in tokens]\n",
    "\n",
    "        #Stemming\n",
    "        if stemmer:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "        # Rejoin tokens\n",
    "\n",
    "        if word2vec:\n",
    "            cleaned_text=tokens\n",
    "        else:\n",
    "            cleaned_text = \" \".join(tokens)\n",
    "\n",
    "        updates.append(cleaned_text)\n",
    "\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa032a7",
   "metadata": {},
   "source": [
    "We choosed to use lemmatization in our approach to reduce the dimension and also because it was more frequently used in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a7e41",
   "metadata": {},
   "source": [
    "__`Step 9`__ Apply the preprocessement to X_train and X_val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e943cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7633/7633 [00:00<00:00, 9374.65it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Brazil's central bank stepped in to prop up th...</td>\n",
       "      <td>brazils central bank stepped prop currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>Singapore Frees Listed Local Developers From H...</td>\n",
       "      <td>singapore frees listed local developers homesa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>$RPAY - Repay Holdings buys Ventanex for up to...</td>\n",
       "      <td>rpay repay holdings buys ventanex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>WHO Pushes Countries to Share More Patient Det...</td>\n",
       "      <td>pushes countries share patient details combat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4654</th>\n",
       "      <td>How clean hydrogen could make the steel indust...</td>\n",
       "      <td>clean hydrogen could make steel industry less ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6226</th>\n",
       "      <td>JPMorgan anticipates ‘disorderly’ year-end fun...</td>\n",
       "      <td>jpmorgan anticipates ‘ disorderly ’ yearend fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9186</th>\n",
       "      <td>$IMMU (+3.2% pre) FDA GRANTS FAST TRACK DESIGN...</td>\n",
       "      <td>immu pre fda grants fast track designation sac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>Hero MotoCorp Q3 Results: Profit Beats Estimat...</td>\n",
       "      <td>hero motocorp q results profit beats estimates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Applied DNA Announces Issuance of U.S. Patent ...</td>\n",
       "      <td>applied dna announces issuance us patent prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>Warren’s Wealth Tax Would Raise Up to $2.7 Tri...</td>\n",
       "      <td>warren ’ wealth tax would raise trillion years...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7633 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "447   Brazil's central bank stepped in to prop up th...   \n",
       "5073  Singapore Frees Listed Local Developers From H...   \n",
       "5941  $RPAY - Repay Holdings buys Ventanex for up to...   \n",
       "5479  WHO Pushes Countries to Share More Patient Det...   \n",
       "4654  How clean hydrogen could make the steel indust...   \n",
       "...                                                 ...   \n",
       "6226  JPMorgan anticipates ‘disorderly’ year-end fun...   \n",
       "9186  $IMMU (+3.2% pre) FDA GRANTS FAST TRACK DESIGN...   \n",
       "3590  Hero MotoCorp Q3 Results: Profit Beats Estimat...   \n",
       "1261  Applied DNA Announces Issuance of U.S. Patent ...   \n",
       "5407  Warren’s Wealth Tax Would Raise Up to $2.7 Tri...   \n",
       "\n",
       "                                                 tokens  \n",
       "447          brazils central bank stepped prop currency  \n",
       "5073  singapore frees listed local developers homesa...  \n",
       "5941                  rpay repay holdings buys ventanex  \n",
       "5479  pushes countries share patient details combat ...  \n",
       "4654  clean hydrogen could make steel industry less ...  \n",
       "...                                                 ...  \n",
       "6226  jpmorgan anticipates ‘ disorderly ’ yearend fu...  \n",
       "9186  immu pre fda grants fast track designation sac...  \n",
       "3590  hero motocorp q results profit beats estimates...  \n",
       "1261  applied dna announces issuance us patent prote...  \n",
       "5407  warren ’ wealth tax would raise trillion years...  \n",
       "\n",
       "[7633 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['tokens'] = preprocess(X_train['text'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bce06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 955/955 [00:00<00:00, 13179.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>What the Fed meeting minutes could say about i...</td>\n",
       "      <td>fed meeting minutes could say interest rates p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>Alibaba's books close early in $13.4 billion H...</td>\n",
       "      <td>alibabas books close early billion hong kong l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Bank of Japan : Accounts (March 20) #BankofJap...</td>\n",
       "      <td>bank japan accounts march</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>Europe's richest man is spending $1 billion on...</td>\n",
       "      <td>europes richest man spending billion departmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>$EFX - Four Chinese military hackers charged i...</td>\n",
       "      <td>efx four chinese military hackers charged equi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615</th>\n",
       "      <td>$AMTD: TD Ameritrade Investor Movement Index: ...</td>\n",
       "      <td>amtd td ameritrade investor movement index imx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6387</th>\n",
       "      <td>President Trump reportedly walks away from vap...</td>\n",
       "      <td>president trump reportedly walks away vaping ban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>Why Hecla Mining Is a Buy</td>\n",
       "      <td>hecla mining buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>News Highlights : Top Energy News of the Day #...</td>\n",
       "      <td>news highlights top energy news day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3538</th>\n",
       "      <td>Tyson Foods Q1 international/other sales rose ...</td>\n",
       "      <td>tyson foods q internationalother sales rose mln</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>955 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "742   What the Fed meeting minutes could say about i...   \n",
       "1218  Alibaba's books close early in $13.4 billion H...   \n",
       "497   Bank of Japan : Accounts (March 20) #BankofJap...   \n",
       "4430  Europe's richest man is spending $1 billion on...   \n",
       "5829  $EFX - Four Chinese military hackers charged i...   \n",
       "...                                                 ...   \n",
       "6615  $AMTD: TD Ameritrade Investor Movement Index: ...   \n",
       "6387  President Trump reportedly walks away from vap...   \n",
       "8125                          Why Hecla Mining Is a Buy   \n",
       "3209  News Highlights : Top Energy News of the Day #...   \n",
       "3538  Tyson Foods Q1 international/other sales rose ...   \n",
       "\n",
       "                                                 tokens  \n",
       "742   fed meeting minutes could say interest rates p...  \n",
       "1218  alibabas books close early billion hong kong l...  \n",
       "497                           bank japan accounts march  \n",
       "4430  europes richest man spending billion departmen...  \n",
       "5829  efx four chinese military hackers charged equi...  \n",
       "...                                                 ...  \n",
       "6615  amtd td ameritrade investor movement index imx...  \n",
       "6387   president trump reportedly walks away vaping ban  \n",
       "8125                                   hecla mining buy  \n",
       "3209                news highlights top energy news day  \n",
       "3538    tyson foods q internationalother sales rose mln  \n",
       "\n",
       "[955 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val['tokens'] = preprocess(X_val['text'])\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f818ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7633/7633 [00:00<00:00, 13193.15it/s]\n",
      "100%|██████████| 955/955 [00:00<00:00, 13895.75it/s]\n",
      "100%|██████████| 955/955 [00:00<00:00, 14906.74it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_cleaned=preprocess(X_train['text'])\n",
    "X_val_cleaned=preprocess(X_val['text'])\n",
    "X_test_cleaned=preprocess(X_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e9671",
   "metadata": {},
   "source": [
    "__`Step 10`__ Apply the preprocessement to df_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923bfedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2388/2388 [00:00<00:00, 12194.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ETF assets to surge tenfold in 10 years to $50...</td>\n",
       "      <td>etf assets surge tenfold years trillion bank a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Here’s What Hedge Funds Think Evolution Petrol...</td>\n",
       "      <td>’ hedge funds think evolution petroleum corpor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$PVH - Phillips-Van Heusen Q3 2020 Earnings Pr...</td>\n",
       "      <td>pvh phillipsvan heusen q earnings preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>China is in the process of waiving retaliatory...</td>\n",
       "      <td>china process waiving retaliatory tariffs impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Highlight: “When growth is scarce, investors s...</td>\n",
       "      <td>highlight “ growth scarce investors seem willi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>2383</td>\n",
       "      <td>$IVC - Invacare Corporation (IVC) CEO Matthew ...</td>\n",
       "      <td>ivc invacare corporation ivc ceo matthew monag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>2384</td>\n",
       "      <td>Domtar EPS misses by $0.05,  revenue in-line</td>\n",
       "      <td>domtar eps misses revenue inline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>2385</td>\n",
       "      <td>India Plans Incentives to Bring In Foreign Man...</td>\n",
       "      <td>india plans incentives bring foreign manufactu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>2386</td>\n",
       "      <td>$NVCR shows institutional accumulation with bl...</td>\n",
       "      <td>nvcr shows institutional accumulation blue sky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>2387</td>\n",
       "      <td>accesso Technology : Form 8.3 - Accesso Techno...</td>\n",
       "      <td>accesso technology form accesso technology gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2388 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  ETF assets to surge tenfold in 10 years to $50...   \n",
       "1        1  Here’s What Hedge Funds Think Evolution Petrol...   \n",
       "2        2  $PVH - Phillips-Van Heusen Q3 2020 Earnings Pr...   \n",
       "3        3  China is in the process of waiving retaliatory...   \n",
       "4        4  Highlight: “When growth is scarce, investors s...   \n",
       "...    ...                                                ...   \n",
       "2383  2383  $IVC - Invacare Corporation (IVC) CEO Matthew ...   \n",
       "2384  2384       Domtar EPS misses by $0.05,  revenue in-line   \n",
       "2385  2385  India Plans Incentives to Bring In Foreign Man...   \n",
       "2386  2386  $NVCR shows institutional accumulation with bl...   \n",
       "2387  2387  accesso Technology : Form 8.3 - Accesso Techno...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     etf assets surge tenfold years trillion bank a...  \n",
       "1     ’ hedge funds think evolution petroleum corpor...  \n",
       "2             pvh phillipsvan heusen q earnings preview  \n",
       "3     china process waiving retaliatory tariffs impo...  \n",
       "4     highlight “ growth scarce investors seem willi...  \n",
       "...                                                 ...  \n",
       "2383  ivc invacare corporation ivc ceo matthew monag...  \n",
       "2384                   domtar eps misses revenue inline  \n",
       "2385  india plans incentives bring foreign manufactu...  \n",
       "2386  nvcr shows institutional accumulation blue sky...  \n",
       "2387  accesso technology form accesso technology gro...  \n",
       "\n",
       "[2388 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['tokens'] = preprocess(df_test['text'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c9c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2388/2388 [00:00<00:00, 13934.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test_cleaned=preprocess(df_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc43706",
   "metadata": {},
   "source": [
    "# 5. Model Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a1cef",
   "metadata": {},
   "source": [
    "We start by training BART on the train dataset before predicting labels for the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034d64a",
   "metadata": {},
   "source": [
    "__`Step 11`__ Set Up the Model (BART + Custom Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BARTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str = \"facebook/bart-large\", num_labels: int = 3, dropout: float = 0.1):\n",
    "        super(BARTSentimentClassifier, self).__init__()\n",
    "        self.bart = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bart.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bart(input_ids=input_ids, attention_mask=attention_mask, return_dict=True,)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  \n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fbc832",
   "metadata": {},
   "source": [
    "__`Step 12`__ Tokenize & Dataset Preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b270f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n",
    "            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n",
    "            \"labels\": torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(X_train_cleaned, y_train.tolist(), tokenizer)\n",
    "val_dataset = TextDataset(X_val_cleaned, y_val.tolist(), tokenizer)\n",
    "test_dataset = TextDataset(X_test_cleaned, y_test.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcadb30",
   "metadata": {},
   "source": [
    "__`Step 13`__ Define the function to train the Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_bart = BARTSentimentClassifier().to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "def train_transformer(train_loader, val_loader, model):\n",
    "\n",
    "  optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "  criterion = CrossEntropyLoss()\n",
    "\n",
    "  for epoch in range(3):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "      for batch in train_loader:\n",
    "          input_ids = batch[\"input_ids\"].to(device)\n",
    "          attention_mask = batch[\"attention_mask\"].to(device)\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(input_ids, attention_mask)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "\n",
    "      print(f\"Epoch {epoch + 1} — Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f833e",
   "metadata": {},
   "source": [
    "__`Step 14`__ Define the function to get the metrics of BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66daf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_metrics_transformers(data_loader, model):\n",
    "  model.eval()\n",
    "  all_preds, all_labels = [], []\n",
    "  with torch.no_grad():\n",
    "      for batch in data_loader:\n",
    "          input_ids = batch[\"input_ids\"].to(device)\n",
    "          attention_mask = batch[\"attention_mask\"].to(device)\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          outputs = model(input_ids, attention_mask)\n",
    "          preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "          all_preds.extend(preds.cpu().numpy())\n",
    "          all_labels.extend(labels.cpu().numpy())\n",
    "      report=classification_report(all_labels, all_preds, target_names=[\"0\", \"1\", \"2\"],output_dict=True, digits=4)\n",
    "\n",
    "      filtered_report = {\n",
    "        label: {\n",
    "            \"precision\": report[label][\"precision\"],\n",
    "            \"recall\": report[label][\"recall\"],\n",
    "            \"f1-score\": report[label][\"f1-score\"]\n",
    "        }\n",
    "        for label in [\"0\", \"1\", \"2\", \"macro avg\"]\n",
    "      }\n",
    "\n",
    "      df_metrics = pd.DataFrame.from_dict(filtered_report, orient=\"index\")\n",
    "      return df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421fe4d7",
   "metadata": {},
   "source": [
    "__`Step 15`__ Train BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a841b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer(train_loader, val_loader, model_bart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f93fd",
   "metadata": {},
   "source": [
    "__`Step 16`__ Get BART metrics in train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39657da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_metrics_transformers(train_loader, model_bart))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32a84b",
   "metadata": {},
   "source": [
    "__`Step 17`__ Get BART metrics in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_metrics_transformers(val_loader, model_bart))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff6847",
   "metadata": {},
   "source": [
    "__`Step 18`__ Get BART metrics in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_metrics_transformers(test_loader, model_bart))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd2da5",
   "metadata": {},
   "source": [
    "# 6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbfdf8",
   "metadata": {},
   "source": [
    "__`Step 19`__ Make predictions in the test dataset and save them in a csv file with the id and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1190ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"Dataset that carries *only* the inputs; dummy label so __getitem__ matches the training collate.\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = enc[\"input_ids\"]\n",
    "        self.attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "        }\n",
    "\n",
    "# Build the dataset and loader\n",
    "infer_ds      = InferenceDataset(df_test[\"tokens\"].tolist(), tokenizer)\n",
    "infer_loader  = DataLoader(infer_ds, batch_size=32)\n",
    "\n",
    "# Run the model\n",
    "model_bart.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in infer_loader:\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        logits   = model_bart(input_ids, attention_mask)\n",
    "        batch_pm = torch.argmax(logits, dim=1)        \n",
    "        preds.extend(batch_pm.cpu().tolist())\n",
    "\n",
    "# Attach predictions & save\n",
    "df_test[\"label\"] = preds              \n",
    "out_cols = [\"id\", \"label\"]         \n",
    "df_test.to_csv(\"pred_42.csv\", columns=out_cols, index=False)\n",
    "\n",
    "print(\"Saved\", len(df_test), \"pred_42.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
